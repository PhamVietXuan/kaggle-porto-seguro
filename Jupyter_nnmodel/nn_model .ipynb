{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#homemade script\n",
    "from util import Gini\n",
    "from feature_generater import Multiply_Divide, Series_string, Features_Counts, Statistic_features\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#for NN model\n",
    "from keras.layers import Dense, Dropout, Embedding, Flatten, Input, Concatenate, merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.models import Model\n",
    "from time import time\n",
    "import datetime\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'Animal': ['Falcon', 'Falcon',\n",
    "                              'Parrot', 'Parrot'],\n",
    "                   'Max Speed': [380., 370., 24., 26.]})\n",
    "df.groupby(['Animal']).list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data #\n",
    "\n",
    "- Create train, test dataset\n",
    "- Create train target label\n",
    "- Create feature object: cat, num, bin, inter\n",
    "- Create feature columns in train: counting of miss values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_only = True\n",
    "save_cv = True\n",
    "\n",
    "#read data\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "train_label = train['target']\n",
    "train_id = train['id']\n",
    "del train['target'], train['id']\n",
    "\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "test_id = test['id']\n",
    "del test['id']\n",
    "\n",
    "\n",
    "\n",
    "#find missing value by each row and recode to column 'missing'\n",
    "train['missing'] = (train==-1).sum(axis=1).astype(float)\n",
    "test['missing'] = (test==-1).sum(axis=1).astype(float)\n",
    "\n",
    "#get all featrue name\n",
    "feature_names = list(train)\n",
    "\n",
    "# extract feature with cat, bin, num, inter\n",
    "cat_fea = [x for x in list(train) if 'cat' in x]\n",
    "bin_fea = [x for x in list(train) if 'bin' in x]\n",
    "num_features = [c for c in list(train) if ('cat' not in c and 'calc' not in c)]\n",
    "inter_fea = [x for x in list(train) if 'inter' in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering #\n",
    "\n",
    "- Tạo các đặc trưng nhân và chia\n",
    "- Đếm các đặc trưng mới\n",
    "- Load các đặc trưng\n",
    "- Create tạo những đặc trưng thống kê\n",
    "- Nối các đặc trưng và chuẩn bị cho quá trình train\n",
    "- Tại Cat_feature cho tầng embeded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Multiply and Devide feature ##\n",
    "- Nhân từng đặc trưng trong danh sách và tạo cột mới vào tập train và test của dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tạo các đặc trưng nhân và chia \n",
    "features= ['ps_car_13', 'ps_ind_03', 'ps_reg_03', 'ps_ind_15', 'ps_reg_01', 'ps_ind_01']#Mấy cái này tác giả chọn được thông qua kinh nghiệm và thử phân tích \n",
    "train, test, MD_features = Multiply_Divide(train, test, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train[MD_features].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Feature of Counts ##\n",
    "1. Tạo các đặc trưng new_ind, new_reg, new_car\n",
    "2. Đếm số số lượng các giá trị riêng biệt của:cat features, new_ind, new_reg and new_car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "create 1_0_1_1..... data as new_xxx\n",
    "\n",
    "new_ind: collect all data from all relative \"ind\" columns, then generate series number\n",
    "\n",
    "new_reg, new_car for train and test data \n",
    "For RNN processing, generating a sequence number\n",
    "'''\n",
    "\n",
    "\n",
    "category_list = ['ind', 'reg', 'car']\n",
    "#add 'new_ind','new_reg','new_car' in train and test dataset\n",
    "train, test = Series_string(train,test,category_list )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train[['new_ind','new_reg','new_car']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "count_features\n",
    "\n",
    "preparing for train[cat_count_features] \n",
    "cat_fea = \n",
    "['ps_ind_02_cat','ps_ind_04_cat','ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_02_cat',\n",
    " 'ps_car_03_cat','ps_car_04_cat','ps_car_05_cat','ps_car_06_cat','ps_car_07_cat',\n",
    " 'ps_car_08_cat','ps_car_09_cat','ps_car_10_cat', 'ps_car_11_cat']\n",
    "\n",
    "Example: \n",
    "ps_ind_02_cat_count\n",
    "dictionay of ps_ind_02_cat \n",
    "([(1, 1079327), (2, 309747), (3, 70172), (4, 28259), (-1, 523)])\n",
    "\n",
    "row        count     origial value\n",
    "595202    1079327       1     \n",
    "595203     309747       2\n",
    "595204     309747       2\n",
    "595205      70172       3\n",
    "595206    1079327       1\n",
    "\n",
    "''' \n",
    "\n",
    "cat_fea = [ name for name in list(train) if 'cat' in name and 'count' not in name]\n",
    "features= cat_fea + ['new_ind','new_reg','new_car']\n",
    "\n",
    "train, test, cat_count_features= Features_Counts(train, test, features)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(train[['new_ind','new_reg','new_car']].head(5))\n",
    "display(train[cat_count_features].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Get the feature from feature training ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fea0, test_fea0 = pickle.load(open(\"../input/fea0.pk\",'rb'), encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Statistic features ##\n",
    "\n",
    "- Tìm mấy thông tin mean, max, median các kiểu. nói chung là thống kê "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature aggregation\n",
    "target_features = ['ps_car_13', 'ps_ind_03', 'ps_reg_03', 'ps_ind_15', 'ps_reg_01', 'ps_ind_01']\n",
    "group_features = ['ps_car_13', 'ps_ind_03', 'ps_reg_03', 'ps_ind_15', 'ps_reg_01', 'ps_ind_01', 'ps_ind_05_cat']\n",
    "\n",
    "#return numpy because we need to do np.hstack to merge all statistic feature together, so that it would return np array\n",
    "train_statis, test_statis =  Statistic_features(train, test, target_features, group_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_statis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Combine all feature together, and get ready for training ##\n",
    "\n",
    "1. merge features into train_list & test_list that would like to dump into NN model\n",
    "2. training a scaler by sparse that generated by train_list & test_list\n",
    "4. convert train_list & test_list into X , X_test, which has been scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Building a train list including train_num, cat_count_features, statistic feature and infered feature training by XGboost.\n",
    "\n",
    "train_num: training data without set of cat_calc\n",
    "cat_count_features: cat_fea + ['new_ind','new_reg','new_car']\n",
    "train_fea0: feature extraction \n",
    "'''\n",
    "\n",
    "\n",
    "#training data without set of cat_calc\n",
    "train_num = train[[x for x in list(train) if x in num_features]]\n",
    "test_num = test[[x for x in list(train) if x in num_features]]\n",
    "\n",
    "train_list = [train_num.replace([np.inf, -np.inf, np.nan], 0), train[cat_count_features], train_statis, train_fea0 ]\n",
    "test_list = [test_num.replace([np.inf, -np.inf, np.nan], 0), test[cat_count_features], test_statis,test_fea0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "X are stacked from 5 features\n",
    "1. train_num(595212,54): training data without set of cat_calc\n",
    "2. cat_count_features(595212,17): cat_fea + ['new_ind','new_reg','new_car']\n",
    "3. feature statis(595212,6) * 36\n",
    "4. train_fea0(595212, 38): feature extraction\n",
    "\n",
    "all_data (595212, 235)\n",
    "'''\n",
    "\n",
    "\n",
    "X = sparse.hstack(train_list).tocsr()\n",
    "X_test = sparse.hstack(test_list).tocsr()\n",
    "\n",
    "all_data = np.vstack([X.toarray(), X_test.toarray()])\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_data)\n",
    "X = scaler.transform(X.toarray())\n",
    "X_test = scaler.transform(X_test.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Create Cat_feature for NN embeding training ##\n",
    " Don't ask why they doing this! they only tell you what is this\n",
    " \n",
    " - in the feature NN model of the finial testing data, you would need the list, which likes **[[cat_featrue], X]**\n",
    " or **[['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat',.....,'ps_car_11_cat'], X]**\n",
    " \n",
    " **_This is to process the above testing data. If you could not understand, that is fine, and just look the next steps_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing for training cat \n",
    "train_cat = train[cat_fea]\n",
    "test_cat = test[cat_fea]\n",
    "\n",
    "# convert pd to np.array\n",
    "X_cat = train_cat.values\n",
    "tem = test_cat.values\n",
    "\n",
    "# storing the dimension for embedding layer as an input value\n",
    "max_cat_values = []\n",
    "\n",
    "for c in cat_fea:\n",
    "    \n",
    "    #nomalize the label\n",
    "    #LabelEncoder: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    x = le.fit_transform(pd.concat([train_cat, test_cat])[c])\n",
    "    train_cat.loc[:,c] = le.transform(train_cat[c])\n",
    "    test_cat.loc[:,c] = le.transform(test_cat[c])\n",
    "    max_cat_values.append(np.max(x))\n",
    "\n",
    "# Build the final testing data\n",
    "X_TEST_CAT = []\n",
    "for i in range(tem.shape[1]):\n",
    "    X_TEST_CAT.append(tem[:, i].reshape(-1, 1))\n",
    "X_TEST_CAT.append(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cat_fea:', cat_fea)\n",
    "print('\\nmax_cat_values: ',max_cat_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training NN Model with Keras # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Build the model\n",
    "2. training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Build the model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model structure:  ###\n",
    "<img src=\"Jupyter_image/NN_layer.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model():\n",
    "    inputs = []\n",
    "    flatten_layers = []\n",
    "    for e, c in enumerate(cat_fea):\n",
    "        input_c = Input(shape=(1, ), dtype='int32')\n",
    "        num_c = max_cat_values[e]\n",
    "        \n",
    "        # need to add 1, https://keras.io/layers/embeddings/\n",
    "        # **input_dim: int > 0. Size of the vocabulary, i.e. maximum integer index + 1.**\n",
    "        embed_c = Embedding(num_c+1,6,input_length=1)(input_c)\n",
    "        embed_c = Dropout(0.25)(embed_c)\n",
    "        flatten_c = Flatten()(embed_c)\n",
    "        inputs.append(input_c)\n",
    "        flatten_layers.append(flatten_c)\n",
    "        \n",
    "    \n",
    "    input_num = Input(shape=(X.shape[1],), dtype='float32')\n",
    "    inputs.append(input_num)\n",
    "    \n",
    "    #merge X and embedding layer\n",
    "    flatten_layers.append(input_num)\n",
    "    flatten = merge(flatten_layers, mode='concat')\n",
    "\n",
    "    fc1 = Dense(512, kernel_initializer='he_normal')(flatten)\n",
    "    fc1 = PReLU()(fc1)\n",
    "    fc1 = BatchNormalization()(fc1)\n",
    "    fc1 = Dropout(0.75)(fc1)\n",
    "\n",
    "    fc1 = Dense(64, kernel_initializer='he_normal')(fc1)\n",
    "    fc1 = PReLU()(fc1)\n",
    "    fc1 = BatchNormalization()(fc1)\n",
    "    fc1 = Dropout(0.5)(fc1)\n",
    "\n",
    "    outputs = Dense(1, kernel_initializer='he_normal', activation='sigmoid')(fc1)\n",
    "\n",
    "    model = Model(inputs = inputs, outputs = outputs)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Start to Train ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#validation fold\n",
    "NFOLDS = 5\n",
    "kfold = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=218)\n",
    "\n",
    "I change \"test\" to \"vaild\" because I feel it is clear to understand\n",
    "\"\"\"\n",
    "\n",
    "cv_train = np.zeros(len(train_label))\n",
    "cv_pred = np.zeros(len(test_id))\n",
    "\n",
    "#validation fold\n",
    "NFOLDS = 5\n",
    "kfold = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=218)\n",
    "\n",
    "#with different random see make result stable.\n",
    "num_seeds = 5\n",
    "begintime = time()\n",
    "if cv_only:\n",
    "    for s in range(num_seeds):\n",
    "        np.random.seed(s)\n",
    "        for (train_index, valid_index) in kfold.split(X, train_label):\n",
    "            \n",
    "            #assign data from training data and labels to validation data; \n",
    "            x_train = X[train_index]\n",
    "            y_train = train_label[train_index]\n",
    "            x_valid= X[valid_index]\n",
    "            y_valid = train_label[valid_index]\n",
    "            \n",
    "            # assign X_cat to validation data; \n",
    "            x_train_cat = X_cat[train_index]\n",
    "            x_valid_cat = X_cat[valid_index]\n",
    "\n",
    "            #Package data for training, the package(list) is  [[cat_featrues], x_train] \n",
    "            # or [ ['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat',.....,'ps_car_11_cat'] ,x_train]\n",
    "            \n",
    "            x_train_cat_list, x_valid_cat_list = [], []\n",
    "            for i in range(x_train_cat.shape[1]):\n",
    "                x_train_cat_list.append(x_train_cat[:, i].reshape(-1, 1))\n",
    "                x_valid_cat_list.append(x_valid_cat[:, i].reshape(-1, 1))\n",
    "\n",
    "            x_train_cat_list.append(x_train)\n",
    "            x_valid_cat_list.append(x_valid)\n",
    "            \n",
    "            #load model\n",
    "            model = nn_model()\n",
    "            \n",
    "            def get_rank(x):\n",
    "                return pd.Series(x).rank(pct=True).values\n",
    "            #fit model. Note: Change epochs to make prediction accuracy\n",
    "            model.fit(x_train_cat_list, y_train, epochs=10, batch_size=512, verbose=2, validation_data=[x_valid_cat_list, y_valid])\n",
    "            \n",
    "            #record prediction with validation data\n",
    "            cv_train[valid_index] += get_rank(model.predict(x=x_valid_cat_list, batch_size=512, verbose=0)[:, 0])\n",
    "            print('local fold Gini: ',Gini(train_label[valid_index], cv_train[valid_index]))\n",
    "            \n",
    "            #recode prediction with testing data\n",
    "            cv_pred += get_rank(model.predict(x=X_TEST_CAT, batch_size=512, verbose=0)[:, 0])\n",
    "             \n",
    "            \n",
    "        \n",
    "        print(\"seed {0}: Gini {1}\".format(s,Gini(train_label, cv_train / (1. * (s + 1)))))\n",
    "        print(\"Total training time: \",str(datetime.timedelta(seconds=time() - begintime)))\n",
    "    if save_cv:\n",
    "        \n",
    "        #divid (NFOLDS * num_seeds) to get average of probablity \n",
    "        pd.DataFrame({'id': test_id, 'target': get_rank(cv_pred * 1./ (NFOLDS * num_seeds))}).to_csv('../model/keras5_pred.csv', index=False)\n",
    "        pd.DataFrame({'id': train_id, 'target': get_rank(cv_train * 1. / num_seeds)}).to_csv('../model/keras5_cv.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
